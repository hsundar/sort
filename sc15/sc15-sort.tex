\documentclass{sig-alternate}

\usepackage{color}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{SC'15}{'15 Austin, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Increasing the Energy Efficiency of Distributed Sorting by Avoiding Communication}

\numberofauthors{5} 

\author{
  -,-,-
}

\maketitle
\begin{abstract}
Sorting is an essential building block for several algorithms and application. In this work we improve the energy efficiency of distributed sorting by proposing a new algorithm that avoids communication whenever possible. In addition, we analyze the most energy and power efficient sorting strategies on GPUs and Heterogeneous system architectures.
\end{abstract}

% \category{H.4}{Information Systems Applications}{Miscellaneous}
% \category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

% \terms{Theory}

% \keywords{ACM proceedings, \LaTeX, text tagging}

\section{Introduction}
General overview on why this work is relavant. Motivate and connect to a larger set of problems. Highlight contributions and list limitations.


\section{Background}
Keep related work here and separate from introduction. some glue text here before we go into separate subsections on the energy aspects and the algorithmic aspects of sorting.

\subsection{energy considerations on GPUs}

\subsection{sorting algorithms}
review popular distributed sorting algorithms. Talk briefly about communication avoiding algorithms and with our prior work on HykSort.

\section{Distributed Sorting}

Here describe the actual sorting algorithms used.

\section{Energy-Efficient Node Sorting}
The previously-described distributed sorting algorithm relies
on a node-level sort of the subset of data assigned to each node at 
each stage of the sort.  This section describes how we arrive 
at a node-level sorting algorithm that is both energy efficient and
high performance.  

{\color{red}Establish that GPU is highest performing and most energy efficient for node sort.
Show performance/energy difference on Jetson compared with OpenMP?}  

\noindent
{\bf Sorting algorithms.}
In this paper, we select among two algorithms to use for
the node-level sort.  
\begin{itemize}
\item \emph{Merge Sort:}
Merge Sort sorts a list of data by 
recursively splitting the list in half, sorting each half,
and then merging the two sorted lists together.
The Merge Sort implementation we use is
part of the ModernGPU~\cite{modernGPU} library of GPU
primitives.  
\item \emph{Radix sort:}
Radix Sort achieves a sorted list by grouping keys by individual digits  
that have the same position and value.
The Radix sort implementation is provided in CUB~\cite{cub}. 
\end{itemize}
{\color{red} May want to also say when one might be preferable to another.}

\noindent
{\bf Managing energy and power on the GPU.} 
We use two mechanisms to adjust energy and power usage on the GPU.  
We can monitor energy or power usage for each of the two 
sort algorithms, and together with performance measurements,
select the preferred algorithm.  In addition, the target Nvidia GPUs
allow adjusting of the clock frequency, or frequency of the memory
bus.  Through monitoring energy or power at different frequency
settings, we can select the preferred frequency(ies).  


\noindent
{\bf Multi-objective tuning.}
Since we would prefer an implementation that is both high performing and
energy efficient, we employ multi-objective tuning, which selects an 
implementation according to both its performance and its energy usage or
peak power requirements.  
{\color{red}What is our selection criteria?}

\noindent
{\bf Code variant selection using Nitro.}
The system described in this paper extends
the Nitro autotuning framework~\cite{muralidharan:2014}.
Nitro provides a library interface that permits expert programmers to
express code variants along with meta-information that aids
the system in selecting among the set of variants at runtime.
Figure~\ref{fig:overview} illustrates the approach in Nitro.  
A learning algorithm -- Support Vector Machine (SVM) classifier by default -- co
nstructs a 
code variant selection
model on the target architecture as a result of an offline training phase on the
 same architecture.
For each architecture, training data has the form
$\{(\mathbf{x}_1, y_1), \hdots, (\mathbf{x}_M, y_M)\}$, where
each $\mathbf{x}_i$ represents an input feature vector and each
$y_i$ represents the best variant for that input.
When presented with a new, unseen input at runtime, the model
predicts the best variant to use. 

In this paper, we extend Nitro in two ways: (1) we treat 
different clock frequencies as code variants, in addition to the
different sorting algorithms; and, (2) the model is trained
using both performance and energy/power data, according to the 
previously-described multi-objective selection criteria.

\section{Experimental Setup}

{\color{red} Talk about architectures used, how we measure energy etc.}

\noindent
{\bf Input data.}
As the performance of sort is dependent on its input data, we 
use a variety of data types, distributions and sizes in our experiments.
We consider XXX data types: 
integer, long integer, float, double, {\color{red} fill in
the rest.}
The distribution of key values 
includes a uniform distribution, a Gaussian distribution, 
{\color{red} fill in
the rest.}
Sizes are motivated by each experiment and the capacity of the
target architecture.  {\color{red} For Titan, ...  
For K20c ... For Jetson ...  Perhaps this goes in the results section.}

\noindent
{\bf Titan (ORNL).} 
We are interested in 
achieving scalable energy efficiency for distributed
sorting on supercomputers, and therefore the target architecture is the
Titan system at Oak Ridge.  Titan has 
{\color{red}... CPUs ... GPUs ...  compiler/OpenMP/MPI installations?}
We would like to measure performance on the GPUs on Titan, but encountered
an obstacle in the software installation.  
The CUDA version used on Titan is CUDA XXX, which is out-of-date with respect to
sorting implementations we are using.  We are also unable to measure
power and energy in the same way on Titan as on the other platforms.
Therefore, we will focus our
Titan experiments on using the OpenMP implementation of sorting, and focus
data gathering on the communication and scaling aspects of our optimizations.
We will then extrapolate
Titan GPU results using other representative clusters.

%\noindent
%{\bf K20 cluster.}
%{\color{red} This is the SCI cluster.  Omit if we don't have results on it.}

\noindent
{\bf Nvidia K20c standalone GPU.}
For the node-level experiments, we used an Nvidia K20c (Kepler 
generation) standalone GPU, representative of the Titan nodes.  
This machine has 13 GPU streaming multiprocessors, 
for a total of 2496 cores, default clock rate of 758 MHz,
XXX GBytes of memory and {\color{red} cache structure.}
It uses CUDA 6.5 and nvcc compiler version XXX.
We use this machine in our experiments because we have complete control 
over its installation, software tools and configuration for each run.

For energy and power measurements, we use {\color{red} 
... identify software power
measurement tool and other details of methodology}.  The K20c has XXX clock frequency settings,
which can be adjusted 
{\color{red} ... say how:
 list frequencies}.  It has two memory frequencies as well, 
but we do not adjust memory frequency in these experiments 
because the lower memory frequency of {\color{red} say what it is}
is far lower than peak of {\color{red} say what it is}
and is therefore going to perform poorly in a bandwidth-limited
algorithm such as sort.

\noindent
{\bf Jetson TK1 cluster.}
We also measured performance of the distributed sorting on an Nvidia 
Jetson TK1 cluster; the nodes of the Jetson are low-power and lightweight,
consisting of a single GPU streaming multiprocessor (Kepler generation)
with 192 cores, and 
four-plus-one ARM cores, where the fifth ARM core is used as a master processor.
The nodes have a unified DRAM of 2 GBytes, which is shared between CPUs and
GPUs, and separate cache structures for CPU and GPU.
The cluster we use in this experiment has {\color{red} XXX} nodes, and
the nodes are connected with {\color{red} ... network details.}
The software installation uses CUDA 6.5, nvcc compiler version 6.5.35,
MPI version 1.6.5, and OpenMP version 3.1.

The power and energy reported for
Jetson are physical measurements  
using the BK Precision's 2138e 4-1/2½ digital multimeter.
We measure the voltage drop across a known precision resistance in series with the Device Under Test (DUT). With a known resistance and measured voltage on that resistance, the current can be obtained with I=V/R. Here, the resistance is 0.020 ohms with a 1\% variation.
To determine the power, P=IV where I is the value calculated above, and V is 12V.
The Jetson has fourteen core clock frequencies ranging from 72MHz to 852MHz,
and twelve memory frequencies from 12.75MHz to 924MHz;
because collecting physical measurements 
on all 1728 combinations of core/memory frequency would 
be prohibitively time-consuming, 
we sampled for this experiment. {\color{red} Perhaps
say what data we have or save for the results.}

While not capable of the high GPU performance of the K20c
since it has only one-thirteenth of the SMs, the
Jetson GPU still outperforms the OpenMP node-level sort by XXX
{\color{red} for a ... explain experiment...}.  
Therefore, while not necessarily representative of Titan, 
the Jetson cluster looks to the future of high-performance and 
embedded GPU platforms.  The unified memory allows us to look 
at power and energy without the data movement required to copy
from CPU to GPU, and the large number of frequency adjustments
allow us to examine how the large number of degrees of freedom
in energy management impacts energy, power and performance in 
code variant selection. 

\section{Experimental Results}

{\color{red} Things to answer.}
\begin{itemize}
\item Are different algorithms affected differently by frequency adjustment?
\item How does frequency affect performance/energy/power?
\item End with SCI cluster and Titan results, and extrapolate from prior 
measurements.
\end{itemize}

\section{Discussion}
Discuss results and what the key findings are and what it means for future architectures.


%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
Thank Christopher Strong, nvidia?, Funding agencies.


%
\bibliographystyle{abbrv}
\bibliography{esort,nitro,tuning} 

%\balancecolumns % GM June 2007
% That's all folks!
\end{document}
