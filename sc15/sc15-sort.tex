\documentclass{sig-alternate}

\usepackage{color}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{SC'15}{'15 Austin, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Increasing the Energy Efficiency of Distributed Sorting by Avoiding Communication}

\numberofauthors{5} 

\author{
  -,-,-
}

\maketitle
\begin{abstract}
Sorting is an essential building block for several algorithms and application. In this work we improve the energy efficiency of distributed sorting by proposing a new algorithm that avoids communication whenever possible. In addition, we analyze the most energy and power efficient sorting strategies on GPUs and Heterogeneous system architectures.
\end{abstract}

% \category{H.4}{Information Systems Applications}{Miscellaneous}
% \category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

% \terms{Theory}

% \keywords{ACM proceedings, \LaTeX, text tagging}

\section{Introduction}
General overview on why this work is relavant. Motivate and connect to a larger set of problems. Highlight contributions and list limitations.


%\section{Background}
%Keep related work here and separate from introduction. some glue text here before we go into separate subsections on the energy aspects and the algorithmic aspects of sorting.
\input{related}

\subsection{energy considerations on GPUs}

\subsection{sorting algorithms}
review popular distributed sorting algorithms. Talk briefly about communication avoiding algorithms and with our prior work on HykSort.

\section{Distributed Sorting}

Here describe the actual sorting algorithms used.

\section{Energy-Efficient Node Sorting}
The previously-described distributed sorting algorithm relies
on a node-level sort of the subset of data assigned to each node at 
each stage of the sort.  This section describes how we arrive 
at a node-level sorting algorithm that is both energy efficient and
high performance.  

\subsection{Overview of Approach}
We first present an overview of the node-level sorting approach, 
looking at factors such as resource selection in a heterogeneous platform, 
sorting algorithms, and how we extend an existing framework for performance
optimization to also take into account energy and power consumption.

\noindent
{\bf GPU vs. Parallel CPU Sorting.}
{\color{red}Establish that GPU is highest performing and most energy efficient for node sort.
Show performance (and possibly energy) 
difference for both Titan and Jetson compared with OpenMP?}  

\vspace*{.1in}

\noindent
{\bf Sorting algorithms as code variants.}
It is well established that the best algorithm for sorting
is dependent on input data set (type, size and distribution), target 
architecture and implementation details.   Therefore, the best-performing
and most energy-efficient implementation cannot
be determined until run time without a prior knowledge of these factors.
We refer to different sorting algorithms or implementations
as \emph{code variants}.  
A number of techniques for code variant selection and algorithm
selection have been described in the literature~\cite{algsel}, and our approach
will rely on recent advancements in this area as described in this section.
 
In this paper, we select among two algorithms to use for
the node-level sort.  
\begin{itemize}
\item \emph{Merge Sort:}
Merge Sort sorts a list of data by 
recursively splitting the list in half, sorting each half,
and then merging the two sorted lists together.
The Merge Sort implementation we use is
part of the ModernGPU~\cite{modernGPU} library of GPU
primitives.  
\item \emph{Radix sort:}
Radix Sort achieves a sorted list by grouping keys by individual digits  
that have the same position and value.
The Radix sort implementation is provided in CUB~\cite{cub}. 
\end{itemize}
{\color{red} May want to also say when one might be preferable to another.}

\vspace*{.1in}

\noindent
{\bf Managing energy and power on the GPU.} 
We use two mechanisms to adjust energy and power usage on the GPU.  
We can monitor energy or power usage for each of the two 
sort algorithms, and together with performance measurements,
select the preferred algorithm.  In addition, the target Nvidia GPUs
allow adjusting of the clock frequency, or frequency of the memory
bus.  Through monitoring energy or power at different frequency
settings, we can select the preferred frequency(ies).  

Since we would prefer an implementation that is both high performing and
power or energy efficient, we must develop a \emph{selection 
criteria} that considers multiple optimization goals in selecting the
node-level sorting algorithm.
%employ multi-objective tuning, which selects an 
%implementation according to both its performance and its energy usage or
%peak power requirements.  
The next subsection will describe
a number of different selection criteria we explore in this paper
and their overall impact on performance, energy and power.

\vspace*{.1in}

\noindent
{\bf Code variant selection using Nitro.}
The system described in this paper used for code variant
selection extends
the Nitro autotuning framework~\cite{muralidharan:2014}.
Nitro provides a library interface that permits expert programmers to
express code variants along with meta-information that aids
the system in selecting among the set of variants at runtime.
Figure~\ref{fig:overview} illustrates the approach in Nitro.  
A learning algorithm -- Support Vector Machine (SVM) classifier by default -- co
nstructs a 
code variant selection
model on the target architecture as a result of an offline training phase on the
 same architecture.
For each architecture, training data has the form
$\{(\mathbf{x}_1, y_1), \hdots, (\mathbf{x}_M, y_M)\}$, where
each $\mathbf{x}_i$ represents an input feature vector and each
$y_i$ represents the best variant for that input.
When presented with a new, unseen input at runtime, the model
predicts the best variant to use. 
For sort, prior work has used data type, data set size and presortedness
as features~\cite{muralidharan:2014}.  In this paper,
we omit presortedness, and replace it with the distribution
of key values.  These three features are available when the
sort is invoked, and can be used in consulting a model for code
variant selection at run time.
In this paper, we extend Nitro in two ways: (1) we treat 
different clock frequencies as code variants, in addition to the
different sorting algorithms; and, (2) the model is trained
using both performance and energy/power data, according to the 
selection criteria outlined in the next subsection.

\subsection{Code Variant Selection Criteria}
Application tuning that looks at multiple optimization criteria
is referred to as \emph{multi-objective tuning}.
A challenge with multi-objective tuning is that the solution must encompass
a tradeoff space between different optimization objectives.  Any 
solution among the \emph{Pareto frontier} is valid; these points are ones
for which there is no other solution that has better metrics among
all the set of objectives.
{\color{red} CLEAN UP AND ADD CITATIONS!!!
Techniques for multi-objective tuning resolve this selection in several
ways:
(1) treat one objective as independent (e.g., an equivalence class
as in PetaBricks paper);  (2) weight one objective above others; 
(3) ask users; (4) use heuristics to drive optimization; or, (5) come up with combined metric.}

In this paper, we have chosen to use a set of fairly standard combined metrics.
We explore which leads to the best reduction in energy or power with the least
impact on performance.  These metrics are as follows: 
\begin{itemize}
\item MKeys per Joule: {\color{red} Define}
\item MKeys$^2$ per Joule: {\color{red} Define}
\item MKeys per Watt: {\color{red} Define}
\item MKeys$^2$ per Watt: {\color{red} Define}
\end{itemize}
These were selected because they capture the relationship between throughput
and energy or power.  Further, it is straightforward to build a model 
for code variant selection by consolidating on a single metric.
In our experiments we will show {\color{red} some indication that this works well.}

\section{Experimental Methodology}

{\color{red} Talk about architectures used, how we measure energy etc.}

\noindent
{\bf Input data.}
As the performance of sort is dependent on its input data, we 
use a variety of data types, distributions and sizes in our experiments.
We consider XXX data types: 
integer, long integer, float, double, {\color{red} fill in
the rest.}
The distribution of key values 
includes a uniform distribution, a Gaussian distribution, 
{\color{red} fill in
the rest.}
Sizes are motivated by each experiment and the capacity of the
target architecture.  {\color{red} For Titan, ...  
For K20c ... For Jetson ...  Perhaps this goes in the results section.}

\noindent
{\bf Titan (ORNL).} 
We are interested in 
achieving scalable energy efficiency for distributed
sorting on supercomputers, and therefore the target architecture is the
Titan system at Oak Ridge.  Titan has 
{\color{red}... CPUs ... GPUs ...  compiler/OpenMP/MPI installations?}
We would like to measure performance on the GPUs on Titan, but encountered
an obstacle in the software installation.  
The CUDA version used on Titan is CUDA XXX, which is out-of-date with respect to
sorting implementations we are using.  We are also unable to measure
power and energy in the same way on Titan as on the other platforms.
Therefore, we will focus our
Titan experiments on using the OpenMP implementation of sorting, and focus
data gathering on the communication and scaling aspects of our optimizations.
We will then extrapolate
Titan GPU results using other representative clusters.

%\noindent
%{\bf K20 cluster.}
%{\color{red} This is the SCI cluster.  Omit if we don't have results on it.}

\noindent
{\bf Nvidia K20c standalone GPU.}
For the node-level experiments, we used an Nvidia K20c (Kepler 
generation) standalone GPU, representative of the Titan nodes.  
This machine has 13 GPU streaming multiprocessors, 
for a total of 2496 cores, 
XXX GBytes of memory and {\color{red} cache structure.}
It uses CUDA 6.5 and nvcc compiler version XXX.
We use this machine in our experiments because we have complete control 
over its installation, software tools and configuration for each run.

For energy and power measurements, we use {\color{red} 
... identify software power
measurement tool and other details of methodology}.  The K20c has five clock frequency settings ranging from 614MHz to 758MHz, with 705MHz as the default setting.
The clock frequency can be adjusted 
{\color{red} ... say how:
 list frequencies}.  It has two memory frequencies as well, 
but we do not adjust memory frequency in these experiments 
because the lower memory frequency of {\color{red} say what it is}
is far lower than peak of {\color{red} say what it is}
and is therefore going to perform poorly in a bandwidth-limited
algorithm such as sort.

\noindent
{\bf Jetson TK1 cluster.}
We also measured performance of the distributed sorting on an Nvidia 
Jetson TK1 cluster; the nodes of the Jetson are low-power and lightweight,
consisting of a single GPU streaming multiprocessor (Kepler generation)
with 192 cores, and 
four-plus-one ARM cores, where the fifth ARM core is used as a master processor.
The nodes have a unified DRAM of 2 GBytes, which is shared between CPUs and
GPUs, and separate cache structures for CPU and GPU.
The cluster we use in this experiment has {\color{red} XXX} nodes, and
the nodes are connected with {\color{red} ... network details.}
The software installation uses CUDA 6.5, nvcc compiler version 6.5.35,
MPI version 1.6.5, and OpenMP version 3.1.

The power and energy reported for
Jetson are physical measurements  
using the BK Precision's 2138e 4-1/2½ digital multimeter.
We measure the voltage drop across a known precision resistance in series with the Device Under Test (DUT). With a known resistance and measured voltage on that resistance, the current can be obtained with I=V/R. Here, the resistance is 0.020 ohms with a 1\% variation.
To determine the power, P=IV where I is the value calculated above, and V is 12V.
The Jetson has fourteen core clock frequencies ranging from 72MHz to 852MHz,
and twelve memory frequencies from 12.75MHz to 924MHz;
because collecting physical measurements 
on all 1728 combinations of core/memory frequency would 
be prohibitively time-consuming, 
we sampled for this experiment. {\color{red} Perhaps
say what data we have or save for the results.}

While not capable of the high GPU performance of the K20c
since it has only one-thirteenth of the SMs, the
Jetson GPU still outperforms the OpenMP node-level sort by XXX
{\color{red} for a ... explain experiment...}.  
Therefore, while not necessarily representative of Titan, 
the Jetson cluster looks to the future of high-performance and 
embedded GPU platforms.  The unified memory allows us to look 
at power and energy without the data movement required to copy
from CPU to GPU, and the large number of frequency adjustments
allow us to examine how the large number of degrees of freedom
in energy management impacts energy, power and performance in 
code variant selection. 

\section{Experimental Results}

{\color{red} Things to answer.}
\begin{itemize}
\item Are different algorithms affected differently by frequency adjustment?
\item How does frequency affect performance/energy/power?
\item End with SCI cluster and Titan results, and extrapolate from prior 
measurements.
\end{itemize}

\section{Discussion}
Discuss results and what the key findings are and what it means for future architectures.


%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
Thank Christopher Strong, nvidia?, Funding agencies.


%
\bibliographystyle{abbrv}
\bibliography{esort,nitro,tuning} 

%\balancecolumns % GM June 2007
% That's all folks!
\end{document}
