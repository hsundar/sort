\section{Background}
Keep related work here and separate from introduction. some glue text here before we go into separate subsections on the energy aspects and the algorithmic aspects of sorting.

\subsection{energy considerations on GPUs}

\subsection{Sorting Algorithms}
The design and implementation of parallel algorithms for sorting is a well studied subject. In our discussion of prior work, we focus our attention to distributed memory algorithms that have been experimentally shown to scale to large core counts and large datasets. We will also discuss related sorting algorithms (albeit not at scale) that study the energy-efficiency of sorting. We begin by a formal definition of distributed sorting.


Given an array $A$ with $N$ keys and an order (comparison) relation, we would like to sort the elements of $A$ in ascending order. In a distributed memory machine with $p$ tasks, every task is assigned an $N/p$-sized block of $A$. Upon completion, task $i$ will have the $i^{\text{th}}$ block of the sorted array. Note that the overall ordering of the keys is determined by the ordering of the tasks as well as the ordering of the keys on each task.  

\paragraph{Distributed sorting}

The most popular algorithm used in actual distributed
sort implementations is SampleSort, originally proposed by Frazer and McKellar\cite{samplesort}. Given $p$ tasks, we
reshuffle the elements of $A$ in $p$ buckets so that the all the
keys in the $i^\text{th}$ bucket are smaller (or equal) than the keys in
the $(i+1)^\text{th}$ bucket. Bucket $i$ is assigned to task $i$ and thus,
once we have shuffled $A$, each task can sort its bucket of
keys in an embarrassingly parallel manner using any local
(shared-memory-parallel) sort algorithm. The challenge is to obtain good load-balancing, i.e., ensuring that each 
task  has roughly the same number of keys, while minimizing communication costs.  

One possible way to reshuffle $A$ is to estimate the boundaries for each bucket, by selecting $p-1$ keys, which we call
``splitters''. This can be done  by sampling a
subset of keys in $A$, sorting them and selecting splitters from that set. Once these $p-1$ splitters have been
selected, a global data exchange takes place to move the original keys of every task to their correct bucket.
An additional local sort is invoked to finalize the output array. SampleSort is well understood. However, its performance is quite sensitive to the selection of splitters, which can result in load imbalance. Most importantly, the final data exchange---requiring $\mathcal{O}(p^2)$ messages---can congest the network.

This has lead to the development of other sort algorithms such as Hyksort\cite{hyksort} that avoid network congestion and show good performance scalability on large clusters. The general is an extension of quicksort on a Hypercube \cite{wagar87} along with an efficient parallel (median) selection algorithm. 

\begin{itemize}
  \item samplesort
  \item histogram sort
  \item hyksort
  \item cloudramsort
  \item GPU, PHI
\end{itemize}

\paragraph{Energy-efficiency}
mention the general approaches. 
