\section{Distributed Sorting}
\label{sec:dsort}

Large-scale distributed sorting, such as those discussed in \S\ref{sec:bg-dist}, have focused on improving sort throughput (number of keys sorted per sec) and parallel scalability. Since, sorting is entirely dominated by data movement, energy is equivalent to the total data communicated and power is proportional to the peak bandwidth utilized. This motivates us to design an algorithm that minimizes the peak-bandwidth requirement as well the overall data communicated. In this section, we first look at the theoretical arguments in this direction, leading to the actual arguments. 

In \cite{hyksort}, we proposed a variant of HyperQuicksort\cite{wagar87} that allowed us to control the algorithm behavior by adjust a parameter $k$. Selecting $k=2$ made the algorithm behave as HyperQuicksort, whereas selecting $k=p$ resulted in behavior similar to Samplesort. The bandwidth term for the algorithm is,
\begin{equation}
  \label{eq:bw}
    k \frac{\log^2 p}{\log k} + \frac{N}{p}.
\end{equation}   
Ignoring the $N/p$ term, we can see that for Samplesort($k=p$) the bandwidth required is $\mathcal{O}(p\log^ p)$, whereas it is only $\mathcal{O}(\log^ p)$ for Hyperquicksort. So clearly, Hyperquicksort is preferable from the bandwidth (and therefor power) perspective. An added benefit is that Hyperquicksort only sends $\mathcal{O}(p)$ message at each stage as opposed to $\mathcal{O}(p^2)$ for Samplesort. While Hyperquicksort does communicate $\mathcal{O}(n\log n)$ data, as compared with $\mathcal{O}(n)$, it's overall performance and scalability is comparable to Samplesort\cite{hyksort}. It is not possible to reduce the bandwidth term for Samplesort, so we modify Hyperquicksort to reduce the amount data it needs to communicate. This is achieved by selectively changing the task-role instead of moving data between tasks. We call this algorithm {\sc SwapRankSort}. This is in principle similar to moving the computation to the data instead of moving the data to the computation. We now elaborate on {\sc SwapRankSort}.

\subsection{{\sc SwapRankSort}}

During each stage of Hyperquicksort, each task exchanges data with another task whose {\em rank}\footnote{a unique identifier assigned to each task.} differs from its own at bit-$k$, where $k$ is the current stage of Hyperquicksort. How much data is exchanged between the tasks depends on the distribution of local-data on each task and the global median of the data (at this level). Analogous to quicksort, the lower-ranked task retains the keys smaller than the median (pivot) and the higher-ranked task retains the higher keys. Depending on the local-distribution of the keys, all the keys ($N/p$) might be exchanged between the tasks. In {\sc SwapRankSort}, we propose a minor modification, where we evaluate the cost of exchanging data for the default case as well as if the ranks of the two tasks were swapped. A swap simply means that a task that would have retained the smaller keys will now retain the larger keys. Clearly that maximum amount of data exchanged in this case is $N/2p$. 

Given that we run this in a distributed setting implies that even a single pair of tasks having a skewed distribution will cause the overall algorithm to exhibit poor performance. Also note that in case of uniformly distributed (amongst the tasks, independent of the data distribution) keys, {\sc SwapRankSort} performs the same as Hyperquicksort with no tasks swapping ranks. The actual swapping of the ranks takes place at the end of each stage when we split the communicator. This is a step that is needed for Hyperquicksort as well, so there is no significant overhead to swapping ranks. 
